<p align="center">
  <img src="https://avatars.githubusercontent.com/u/12619994?s=200&v=4" width="150">
  <br />
  <a href="LICENSE"><img alt="Apache License" src="https://img.shields.io/badge/License-Apache%202.0-blue.svg" /></a>
</p>

--------------------------------------------------------------------------------

# KD-NLP
This repository is a collection of Knowledge Distillation (KD) methods implemented by the Huawei Montreal NLP team. 

<details><summary>Included Projects</summary><p>

* [**MATE-KD**](MATE-KD)
    * KD for model compression and study of use of adversarial training to improve student accuracy using just the logits of the teacher as in standard KD.
    * [MATE-KD: Masked Adversarial TExt, a Companion to Knowledge Distillation](https://arxiv.org/abs/2105.05912v1)
* [**Combined-KD**](Combined-KD)
    * Proposition of Combined-KD (ComKD) that takes advantage of data-augmentation and progressive training.
    * [How to Select One Among All? An Extensive Empirical Study Towards the Robustness of Knowledge Distillation in Natural Language Understanding
](https://arxiv.org/abs/2109.05696v1)

</p></details>

# License
This project's license is under the Apache 2.0 license.
